{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "This is the stage in which we cook the ML solution. But just like cooking, we're gonna need some good ingredients first. Let's get our hands on what we have prepared so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required library imports & initial settings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# set seed value for reproducibility\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>cat11</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310061</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885834</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422268</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704268</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat4 cat5 cat6 cat8 cat9 cat10 cat11  ...     cont5     cont6  \\\n",
       "0   1    A    B    B    A    A    A    B     A     B  ...  0.310061  0.718367   \n",
       "1   2    A    B    A    A    A    A    B     B     A  ...  0.885834  0.438917   \n",
       "2   5    A    B    A    B    A    A    B     B     B  ...  0.397069  0.289648   \n",
       "3  10    B    B    B    A    A    A    B     A     A  ...  0.422268  0.440945   \n",
       "4  11    A    B    B    A    A    A    B     B     A  ...  0.704268  0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont13    cont14     loss  \n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.822493  0.714843  2213.18  \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.611431  0.304496  1283.60  \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.195709  0.774425  3005.09  \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.605077  0.602642   939.85  \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.246011  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define relative data path (according the current path of this notebook)\n",
    "DATA_PATH = './script/data/'\n",
    "\n",
    "df_train_full = pd.read_csv(DATA_PATH+'train_cleaned.csv.gz')\n",
    "df_test_full  = pd.read_csv(DATA_PATH+'test_cleaned.csv.gz')\n",
    "\n",
    "df_train_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify feature groups\n",
    "features_numerical   = [column for column in df_train_full if column.startswith('cont')]\n",
    "features_categorical = [column for column in df_train_full if column.startswith('cat')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Target Column\n",
    "We noticed in EDA stage that our target column in data could benefit from a log-transform. We'll apply that transform right here in the beginning before any splits, since we will be using stratified method and it is important for us to consider equal distribution of samples in all data chops, according to the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='loss', ylabel='Count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEGCAYAAAB2EqL0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZbklEQVR4nO3dfZAd1X3m8e8zIyQc20GSmai0kioSZVWyOLsGZcyLcRIHEiGIKyJbhMghYZCVqBITl1+27IX1bpHYoSqOXbHBSQAZBJILW5axCQomZrUCGxLHQkOEeVc0hhCkAjRGAkcYMTP3/vJHn5FaYu5Mz+j2fZn7fKpuTffpc7tP3565z3SfflFEYGZmNpGuZjfAzMzagwPDzMwKcWCYmVkhDgwzMyvEgWFmZoXMaHYDynDyySfH4sWLm90MM7O28tBDD/0oInpqTZ+WgbF48WL6+/ub3Qwzs7Yi6dnxpvuQlJmZFeLAMDOzQhwYZmZWiAPDzMwKcWCYmVkhDgwzMyvEgWFmZoU4MMzMrBAHhpmZFeLAqKFSqVCpVJrdDDOzluHAMDOzQhwYNXgPw8zsaA4MMzMrxIFhZmaFODDGEBFUKhUiotlNMTNrGaUGhqTZkm6X9JSkJyWdLWmupK2Sdqefc1JdSbpO0oCkRyQty82nL9XfLamvzDYDVKtVLr3xAarVatmLMjNrG2XvYVwLfDsifh54J/AkcCWwLSKWAtvSOMAFwNL0WgtcDyBpLnA1cCZwBnD1aMiUSV3e+TIzyyvtW1HSScAvAzcDRMRQRLwMrAQ2pGobgIvS8EpgY2S+D8yWNB84H9gaEfsj4gCwFVhRVrvNzGxsZf4bvQQYBG6RtFPSTZLeDMyLiOdTnReAeWl4AfBc7v17Ulmt8qNIWiupX1L/4OBgnVfFzMzKDIwZwDLg+og4HXiVI4efAIisV7kuPcsRsS4ieiOit6en5jPMi8+v6uswzMzyygyMPcCeiNiexm8nC5AX06Em0s99afpeYFHu/QtTWa1yMzNroNICIyJeAJ6T9HOp6DzgCWALMHqmUx9wZxreAlyWzpY6C3glHbq6B1guaU7q7F6eyszMrIFmlDz/DwG3SZoJPA2sJgupzZLWAM8Cl6S6dwMXAgPAT1JdImK/pE8DO1K9T0XE/pLbbWZmxyg1MCLiYaB3jEnnjVE3gCtqzGc9sL6ujTMzs0nxxQZmZlaIA6MG3x7EzOxoDoxaosrqW/t9exAzs8SBMQ7fHsTM7Ah/I5qZWSEODDMzK8SBMYass7vZrTAzay0ODDMzK8SBMQ7fgNDM7AgHhpmZFeLAMDOzQhwYZmZWiANjHO7DMDM7woFhZmaFODDG4T0MM7MjHBhmZlaIA8PMzApxYJiZWSEODDMzK8SBYWZmhTgwzMysEAeGmZkV4sAwM7NCHBhmZlZIqYEh6d8kPSrpYUn9qWyupK2Sdqefc1K5JF0naUDSI5KW5ebTl+rvltRXZpvzIiI9fc+P3zMza8Qexq9GxGkR0ZvGrwS2RcRSYFsaB7gAWJpea4HrIQsY4GrgTOAM4OrRkCldVFl9az/VarUhizMza2XNOCS1EtiQhjcAF+XKN0bm+8BsSfOB84GtEbE/Ig4AW4EVjWqsunzUzswMyg+MAP6fpIckrU1l8yLi+TT8AjAvDS8Ansu9d08qq1V+FElrJfVL6h8cHKznOpiZGTCj5Pm/JyL2SvoZYKukp/ITIyIk1aWDICLWAesAent73elgZlZnpe5hRMTe9HMfcAdZH8SL6VAT6ee+VH0vsCj39oWprFa5mZk1UGmBIenNkt46OgwsBx4DtgCjZzr1AXem4S3AZelsqbOAV9Khq3uA5ZLmpM7u5anMzMwaqMxDUvOAOySNLucrEfFtSTuAzZLWAM8Cl6T6dwMXAgPAT4DVABGxX9KngR2p3qciYn+J7TYzszGUFhgR8TTwzjHKXwLOG6M8gCtqzGs9sL7ebTQzs+J8zugE/JhWM7OMA2MMlUoFfHW3mdlRHBhmZlaIA8PMzApxYJiZWSEOjAm409vMLOPAMDOzQhwYZmZWiAPDzMwKcWCYmVkhDgwzMyvEgWFmZoU4MCbg02rNzDIODDMzK8SBYWZmhTgwzMysEAeGmZkV4sAwM7NCHBhmZlaIA8PMzApxYJiZWSEOjAn4wj0zs4wDw8zMCnFgmJlZIaUHhqRuSTsl3ZXGl0jaLmlA0tckzUzls9L4QJq+ODePq1L5Lknnl91mMzN7o0bsYXwYeDI3/hng8xHxduAAsCaVrwEOpPLPp3pIOhVYBbwDWAH8raTuBrQbgIigUqkQEY1apJlZSyo1MCQtBH4DuCmNCzgXuD1V2QBclIZXpnHS9PNS/ZXApoh4PSKeAQaAM8ps91GiypqNO6lWqw1bpJlZKyp7D+MLwCeA0W/btwEvR8RIGt8DLEjDC4DnANL0V1L9w+VjvOcwSWsl9UvqHxwcrOtKqMtdPWZmpX0TSnofsC8iHiprGXkRsS4ieiOit6enpxGLNDPrKDNKnPc5wG9KuhA4Efhp4FpgtqQZaS9iIbA31d8LLAL2SJoBnAS8lCsflX+PmZk1SGl7GBFxVUQsjIjFZJ3W90bEpcB9wMWpWh9wZxreksZJ0++NrKd5C7AqnUW1BFgKPFhWu83MbGxl7mHU8r+ATZL+HNgJ3JzKbwa+LGkA2E8WMkTE45I2A08AI8AVEdHQS69Hr/bu7m7YyVlmZi2nIYEREd8BvpOGn2aMs5wi4hDw2zXefw1wTXktNDOzifj0HzMzK8SBYWZmhTgwzMysEAeGmZkV4sAwM7NCCgWGpHOKlJmZ2fRVdA/jiwXLzMxsmhr3OgxJZwPvBnokfSw36aeBjrmKzY9pNTOb+MK9mcBbUr235sp/zJHbe5iZWQcYNzAi4rvAdyXdGhHPNqhNLcd7GGZmxW8NMkvSOmBx/j0RcW4ZjTIzs9ZTNDC+DtxA9uQ8/6ttZtaBigbGSERcX2pLzMyspRU9rfbvJX1Q0nxJc0dfpbbMzMxaStE9jNEHG308VxbAKfVtjpmZtapCgRERS8puiJmZtbZCgSHpsrHKI2JjfZtjZmatqughqXflhk8EzgP+BXBgmJl1iKKHpD6UH5c0G9hURoNaUURQqVSICCQ1uzlmZk0x1dubvwp0Tr9GVFl9az/VarXZLTEza5qifRh/T3ZWFGQ3HfyvwOayGtWK1OVHh5hZZyvah/G53PAI8GxE7CmhPWZm1qIK/ducbkL4FNkda+cAQ2U2qhX5BoRm1umKPnHvEuBB4LeBS4Dtknx7czOzDlL0wPwngXdFRF9EXAacAfzf8d4g6URJD0r6gaTHJf1ZKl8iabukAUlfkzQzlc9K4wNp+uLcvK5K5bsknT+lNTUzs+NSNDC6ImJfbvylAu99HTg3It4JnAaskHQW8Bng8xHxduAAsCbVXwMcSOWfT/WQdCqwCngHsAL4W0kd87Q/M7NWUTQwvi3pHkmXS7oc+BZw93hviMzBNHpCegVwLnB7Kt8AXJSGV6Zx0vTzlF30sBLYFBGvR8QzwADZHo6ZmTXQuIEh6e2SzomIjwM3Av89vf4ZWDfRzCV1S3oY2AdsBX4IvBwRI6nKHmBBGl4APAeQpr8CvC1fPsZ78staK6lfUv/g4OBETZs0d3qbWaebaA/jC2TP7yYivhkRH4uIjwF3pGnjiohKRJwGLCTbK/j542nsBMtaFxG9EdHb09NzXPPKgiEmrGdm1kkmCox5EfHosYWpbHHRhUTEy8B9wNnAbEmj138sBPam4b3AIoA0/SSyvpLD5WO8x8zMGmSiwJg9zrQ3jfdGST3pnlNIehPw68CTZMExekpuH3BnGt7CkeduXAzcGxGRylels6iWAEvJTvE1M7MGmuhK735JfxgRX8oXSvoD4KEJ3jsf2JDOaOoCNkfEXZKeADZJ+nNgJ3Bzqn8z8GVJA8B+sjOjiIjHJW0GniC7yvyKiGh4Z4L7MMys000UGB8B7pB0KUcCoheYCfzWeG+MiEeA08cof5oxznKKiENkFwaONa9rgGsmaKuZmZVo3MCIiBeBd0v6VeAXUvG3IuLe0ltmZmYtpejzMO4j63swM7MO5Xt2F5R/iJKZWSdyYBTlhyiZWYdzYEyCH6JkZp3M34BmZlaIA8PMzApxYJiZWSEODDMzK8SBMQm+PYiZdTIHhpmZFeLAmATvYZhZJ3NgmJlZIQ4MMzMrxIFhZmaFODAmwX0YZtbJHBhmZlaIA8PMzApxYJiZWSEOjElwH4aZdTIHhpmZFeLAmAQ/ptXMOpkDYzKiypqNO/2YVjPrSA6MSQv3Y5hZRyotMCQtknSfpCckPS7pw6l8rqStknann3NSuSRdJ2lA0iOSluXm1Zfq75bUV1abzcystjL3MEaA/xkRpwJnAVdIOhW4EtgWEUuBbWkc4AJgaXqtBa6HLGCAq4EzgTOAq0dDxszMGqe0wIiI5yPiX9LwfwBPAguAlcCGVG0DcFEaXglsjMz3gdmS5gPnA1sjYn9EHAC2AivKareZmY2tIX0YkhYDpwPbgXkR8Xya9AIwLw0vAJ7LvW1PKqtVfuwy1krql9Q/ODhY3xUwM7PyA0PSW4BvAB+JiB/np0V2fmpdzlGNiHUR0RsRvT09PfWY5djL8cV7ZtahSg0MSSeQhcVtEfHNVPxiOtRE+rkvle8FFuXevjCV1So3M7MGKvMsKQE3A09GxF/lJm0BRs906gPuzJVfls6WOgt4JR26ugdYLmlO6uxensqawnsYZtapZpQ473OA3wcelfRwKvvfwF8AmyWtAZ4FLknT7gYuBAaAnwCrASJiv6RPAztSvU9FxP4S221mZmMoLTAi4h8B1Zh83hj1A7iixrzWA+vr17qpy98eJNuJMjPrDL7S+xijgVCzKz6qrL6137cHMbOO48A4RrVape9LDxDjnLylLn9sZtZ5/M03BgeCmdkb+ZtxCnymlJl1IgeGmZkV4sCYAu9hmFkncmBMQbUywtDQkJ+8Z2YdxYExFVHl8vXbGR4ebnZLzMwaxoExRT6Tysw6jb/1zMysEAeGmZkV4sAwM7NCHBhT5FNrzazTODCmyIFhZp3GgWFmZoU4MKYo/1wMM7NO4MCYKj8Xw8w6jAPjOPjiPTPrJP7GOw7u+DazTuLAOA6Vodd47bXXmt0MM7OGcGAcB3d8m1kncWAcj6iyZuNOd3ybWUdwYBwnd3ybWafwt52ZmRVSWmBIWi9pn6THcmVzJW2VtDv9nJPKJek6SQOSHpG0LPeevlR/t6S+stprZmbjK3MP41ZgxTFlVwLbImIpsC2NA1wALE2vtcD1kAUMcDVwJnAGcPVoyLQKn1prZp2itMCIiPuB/ccUrwQ2pOENwEW58o2R+T4wW9J84Hxga0Tsj4gDwFbeGEJNVR0ZYmhoqNnNMDMrXaP7MOZFxPNp+AVgXhpeADyXq7cnldUqfwNJayX1S+ofHByccgOz02SL1/eptWbWKZrW6R3ZN2zdvmUjYl1E9EZEb09PT71mW2DBvqeUmXWGRgfGi+lQE+nnvlS+F1iUq7cwldUqbzHhfgwzm/YaHRhbgNEznfqAO3Pll6Wzpc4CXkmHru4Blkuakzq7l6eyluKObzPrBDPKmrGkrwLvBU6WtIfsbKe/ADZLWgM8C1ySqt8NXAgMAD8BVgNExH5JnwZ2pHqfiohjO9KbrloZYWhoiBNPPBFJzW6OmVkpSguMiHh/jUnnjVE3gCtqzGc9sL6OTau/1I/x9T95L93d3c1ujZlZKXyld924H8PMpjcHhpmZFeLAqJPK8Ou89tprvh7DzKYtB0a9RJXL129neHi42S0xMyuFA6NOolo5fNW3mdl05MCoI1+PYWbTmQOjjnxfKTObzhwY9eR+DDObxhwYdRbVEd/u3MymJQdGnUUEQ0NDjIyMNLspZmZ15cCot6jygVt8WMrMph8HRgkiqgwNDbnz28ymFQdGGdz5bWbTkAOjJJXhQxw8eNB7GWY2bTgwjlGpVJjUQ71riSprNu70o1vNbNpwYJSoWhn2DQnNbNpwYJQoKsP8/pe+574MM5sWHBglq1aGOXjwoK/LMLO258AoW1S5/KZ/4tVXX/WhKTNraw6MRogKv3fD/ezfv5/XX3/dwWFmbcmB0SASrL7pH7nki9u8t2FmbcmB0UDq6iIqQ/zeuu8xNDTkW6GbWVtxYBwjewBSuV/i1cowBw4c4OLr7uXQoUNUq1WHh5m1PAdGM0SVtRu2Ux05xKq/uY+XX36Zi7+YhYdDw8xaVdsEhqQVknZJGpB0ZbPbc7zUlX30URmib90DVIcPccm19zA4OMihQ4cYHh4+/BoZGfFeiJk13YxmN6AISd3A3wC/DuwBdkjaEhFP1HtZzfhSHg0PCdas/2ekrDyqFdQ9k+4TZvLlP3w3fbfs4Ctr301XV+2cl0RXV9fhW5J0d3cDUK1W6erqQqMzNzObpLYIDOAMYCAingaQtAlYCdQ9MAAid/+nqFazb3IdPTzVaRPX6z6mLRUqQ6+x6tp/QN0zufivvnW4HHUfDhd1dSMJdXWz/gNns/rm79F9wiw2ffBXqFQqXLrue2z64186HCBmNj2V+TfeLoGxAHguN74HODNfQdJaYG0aPShp13Es72TgR8fx/qZ628cPD54866NH1mPWR5rRmrpo6+2RM13WA6bPung9jvaz401sl8CYUESsA9bVY16S+iOitx7zaiavR2uZLusB02ddvB6T0y6d3nuBRbnxhanMzMwapF0CYwewVNISSTOBVcCWJrfJzKyjtMUhqYgYkfQnwD1AN7A+Ih4vcZF1ObTVArwerWW6rAdMn3XxekyCfF6/mZkV0S6HpMzMrMkcGGZmVogDI6cVbz8iaZGk+yQ9IelxSR9O5XMlbZW0O/2ck8ol6bq0Do9IWpabV1+qv1tSX678FyU9mt5znUq8HFxSt6Sdku5K40skbU/L/lo6qQFJs9L4QJq+ODePq1L5Lknn58obtv0kzZZ0u6SnJD0p6ex23CaSPpp+rx6T9FVJJ7bDNpG0XtI+SY/lykr//Gsto4R1+Wz63XpE0h2SZuemTeqznsr2rCki/Mr6cbqBHwKnADOBHwCntkC75gPL0vBbgX8FTgX+ErgylV8JfCYNXwj8A9m15GcB21P5XODp9HNOGp6Tpj2Y6iq994IS1+djwFeAu9L4ZmBVGr4B+OM0/EHghjS8CvhaGj41bZtZwJK0zbobvf2ADcAfpOGZwOx22yZkF8Q+A7wpty0ub4dtAvwysAx4LFdW+udfaxklrMtyYEYa/kxuXSb9WU92e47b1rL+oNrtBZwN3JMbvwq4qtntGqOdd5LdU2sXMD+VzQd2peEbgffn6u9K098P3JgrvzGVzQeeypUfVa/ObV8IbAPOBe5Kf4w/yv1hHN4GZGfEnZ2GZ6R6Ona7jNZr5PYDTiL7otUx5W21TThyB4W56TO+Czi/XbYJsJijv2RL//xrLaPe63LMtN8CbhvrM5zos57K39h47fQhqSPGuv3Igia1ZUxpl/F0YDswLyKeT5NeAOal4VrrMV75njHKy/AF4BPA6M263ga8HBEjYyz7cHvT9FdS/cmuXxmWAIPALcoOr90k6c202TaJiL3A54B/B54n+4wfoj23CTTm86+1jDJ9gGwvBya/LlP5G6vJgdEmJL0F+AbwkYj4cX5aZP8itPT50ZLeB+yLiIea3ZY6mEF2COH6iDgdeJXs8MRhbbJN5pDdxHMJ8F+ANwMrmtqoOmnE59+IZUj6JDAC3FbmcopyYBzRsrcfkXQCWVjcFhHfTMUvSpqfps8H9qXyWusxXvnCMcrr7RzgNyX9G7CJ7LDUtcBsSaMXkOaXfbi9afpJwEsTrEejtt8eYE9EbE/jt5MFSLttk18DnomIwYgYBr5Jtp3acZtAYz7/WsuoO0mXA+8DLk3hxARtHqv8JSa/PWur93HRdn2R/df4NNl/W6OdRu9ogXYJ2Ah84Zjyz3J059tfpuHf4OgOvgdT+Vyy4+5z0usZYG6admwH34Ulr9N7OdLp/XWO7pD7YBq+gqM75Dan4XdwdKff02Qdfg3dfsADwM+l4T9N26OttgnZHZ8fB34qLWcD8KF22Sa8sQ+j9M+/1jJKWJcVZI9v6Dmm3qQ/68luz3HbWdYfVDu+yM6m+Feysw0+2ez2pDa9h2y39xHg4fS6kOxY4zZgN/D/c7/oInvY1A+BR4He3Lw+AAyk1+pceS/wWHrPXzNBx1cd1um9HAmMU9If50D6xZ6Vyk9M4wNp+im5938ytXUXubOHGrn9gNOA/rRd/i594bTdNgH+DHgqLevL6Yuo5bcJ8FWyfpdhsj2+NY34/Gsto4R1GSDrX3g4vW6Y6mc9le1Z6+Vbg5iZWSHuwzAzs0IcGGZmVogDw8zMCnFgmJlZIQ4MMzMrxIFhVkeSDja7DWZlcWCYmVkhDgyzEqRnMHw2PWfiUUm/k8rnS7pf0sNp2i8pe0bIrbm6H212+83GMmPiKmY2Bf+D7GrwdwInAzsk3Q/8Ltntpa+R1E12W47TgAUR8QuQPZypGQ02m4j3MMzK8R7gqxFRiYgXge8C7wJ2AKsl/Snw3yLiP8juAXSKpC9KWgH8uNZMzZrJgWHWQBFxP9kT1vYCt0q6LCIOkO2JfAf4I+Cm5rXQrDYHhlk5HgB+J/VP9JCFxIOSfhZ4MSK+RBYMyySdDHRFxDeA/0N2q3SzluM+DLNy3EH2OMwfkN1t+BMR8YKkPuDjkoaBg8BlZE8+u0XS6D9wVzWjwWYT8d1qzcysEB+SMjOzQhwYZmZWiAPDzMwKcWCYmVkhDgwzMyvEgWFmZoU4MMzMrJD/BHUIUzhDbCs2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check-out the target column value distribution before log-transform\n",
    "\n",
    "name_of_target_column = 'loss'\n",
    "sns.histplot(df_train_full[name_of_target_column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_target_column_transformed = name_of_target_column+'_transformed'\n",
    "\n",
    "# logarithmic transform function\n",
    "def log_transform(value):\n",
    "    return np.log1p(value)\n",
    "\n",
    "# create a new logarithmically transformed target column\n",
    "df_train_full[name_of_target_column_transformed] = df_train_full.apply(\n",
    "    lambda row: log_transform(row[name_of_target_column]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='loss_transformed', ylabel='Count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check value distribution of transformed target column\n",
    "\n",
    "sns.histplot(df_train_full[name_of_target_column_transformed])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "We have a pretty sizable dataset with 180k+ rows. Needless to say, creating multiple test models and tuning them will take a long time. What should we do then?<br>\n",
    "One simple trick I use in similar cases is to create a subsample dataset with smaller number of samples and apply and adjust training and tuning on this one. Later I can use the full train data and utilize the parameters I discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's shuffle the whole dataframe before subsampling\n",
    "df_train_full = df_train_full.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# second shuffle with an exponential random seed :D\n",
    "df_train_full = df_train_full.sample(frac=1, random_state=(RANDOM_SEED**2)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the subsampled data is similar to our original full dataset, it is advised widely to use __stratified__ method in all sampling scenarios.\n",
    "\n",
    "But we know that stratified sampling works only on classification problems, which have a target column with classes. Our problem here is of regression type and the target column contains continuous numerical values.\n",
    "\n",
    "__So, what should we do to randomly subsample a fraction of data with samples that their target column values conform to distribution pattern in our original dataset?__<br>\n",
    "Pandas _qcut()_ function to the rescue! We create discrete bins (groups) for our target column with (nearly) equal value counts in each bin, and do the subsampling using those bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_target_column_binned = name_of_target_column+'_bin'\n",
    "\n",
    "# 10 bin groups for target values should work fine\n",
    "bin_counts = 10\n",
    "\n",
    "# create bin column in our dataframe\n",
    "df_train_full[name_of_target_column_binned] = pd.qcut(\n",
    "    df_train_full[name_of_target_column_transformed],\n",
    "    q=bin_counts,\n",
    "    labels=list(range(bin_counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think a number around 25k samples is a decent chunk for a (train + validation) subsample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_subsample_size = 25000\n",
    "subsample_fold_split = int(len(df_train_full)/desired_subsample_size)\n",
    "\n",
    "# create desired kfold splitter\n",
    "skf_subsample_full = StratifiedKFold(\n",
    "    n_splits=subsample_fold_split, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# make sure target distribution remains the same by utilizing StratifiedKFold data split\n",
    "train_indices_remaining, train_indices_subsample = next(\n",
    "    skf_subsample_full.split(X=df_train_full, y=df_train_full[name_of_target_column_binned]), 0)\n",
    "\n",
    "df_train_full_subsample = df_train_full.iloc[train_indices_subsample, :].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now separate the full subsample dataset into \"subsample train\" and \"subsample validation\" in similar manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf_subsample_trainvalid = StratifiedKFold(\n",
    "    n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "train_indices_subsample, valid_indices_subsample = next(\n",
    "    skf_subsample_trainvalid.split(X=df_train_full_subsample, y=df_train_full_subsample[name_of_target_column_binned]), 0)\n",
    "\n",
    "df_train_subsample = df_train_full.iloc[train_indices_subsample, :].reset_index(drop=True)\n",
    "df_valid_subsample = df_train_full.iloc[valid_indices_subsample, :].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check to see how closely the subsample (and it's train/valid splits) represents our full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plot_bins = 250\n",
    "\n",
    "plt.hist(\n",
    "    x=df_train_full[name_of_target_column_transformed],\n",
    "    bins=plot_bins,\n",
    "    alpha=0.6,\n",
    "    label='Full Train Dataset')\n",
    "plt.hist(\n",
    "    x=df_train_full_subsample[name_of_target_column_transformed],\n",
    "    bins=plot_bins,\n",
    "    alpha=0.6,\n",
    "    label='Full Subsample Dataset')\n",
    "plt.hist(\n",
    "    x=df_train_subsample[name_of_target_column_transformed],\n",
    "    bins=plot_bins,\n",
    "    alpha=0.5,\n",
    "    label='Subsample Train')\n",
    "plt.hist(\n",
    "    x=df_valid_subsample[name_of_target_column_transformed],\n",
    "    bins=plot_bins,\n",
    "    alpha=0.5,\n",
    "    label='Subsample Validation')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Distribution of Target Column Values')\n",
    "plt.rcParams['figure.figsize'] = (1, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions look pretty similar. That's great news!<br>\n",
    "__Note:__ The _'Y'_ values (indicating sample counts) in above plot are logarithmically scaled for easier comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done with splits, it's time to build out the dataframes containing only feature columns, and of course the target column series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all = features_numerical + features_categorical\n",
    "\n",
    "X_train_subsample = df_train_subsample[features_all].copy()\n",
    "X_valid_subsample = df_valid_subsample[features_all].copy()\n",
    "\n",
    "y_train_subsample = df_train_subsample[name_of_target_column_transformed].to_numpy()\n",
    "y_valid_subsample = df_valid_subsample[name_of_target_column_transformed].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Numerical Features\n",
    "During EDA we noticed the following characteristics for numerical features:<br>\n",
    "- Gaussian-like: _['cont1', 'cont2', 'cont3', 'cont6', 'cont7', 'cont9', 'cont11', 'cont12']_<br>\n",
    "- Non-Gaussian-like: _['cont4', 'cont5', 'cont8', 'cont10', 'cont13', 'cont14']_\n",
    "\n",
    "As a rule of thumb, we'll use min-max scaler for non-gaussian features, and standard scaler for gaussian-like features.\n",
    "\n",
    "You should remember that we dropped some columns in data cleaning step. We should adjust column names in above groups, having that in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_like = ['cont1', 'cont2', 'cont3', 'cont6', 'cont7', 'cont9', 'cont11', 'cont12']\n",
    "non_gaussian_like = ['cont4', 'cont5', 'cont8', 'cont10', 'cont13', 'cont14']\n",
    "\n",
    "features_numerical_to_normalize = [column for column in gaussian_like if column in df_train_subsample.columns.to_list()]\n",
    "features_numerical_to_standardize = [column for column in non_gaussian_like if column in df_train_subsample.columns.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical Features\n",
    "We talked about the curse of dimensionality in EDA stage. For categorical columns, if the cardinality of a column is below 10, we'll nominate it for one-hot encoding; otherwise it goes to ordinal encoding list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_categorical_to_ordinal = list()\n",
    "features_categorical_to_onehot  = list()\n",
    "\n",
    "for column, variety in X_train_subsample[features_categorical].nunique().iteritems():\n",
    "    if variety < 10: features_categorical_to_onehot.append(column)\n",
    "    else: features_categorical_to_ordinal.append(column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's utilize __sk-learn's pipeline__ feature to do the transformations in an efficient way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transform pipeline for numerical features\n",
    "transformer_numerical = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('normalizer', MinMaxScaler()),\n",
    "    ('standardizer', StandardScaler()),\n",
    "])\n",
    "\n",
    "# create transform pipelines for categorical features\n",
    "transformer_categorical_1 = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "    ('normal',  MinMaxScaler()),\n",
    "])\n",
    "transformer_categorical_transformer2 = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "# bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',  transformer_numerical, features_numerical),\n",
    "        ('cat1', transformer_categorical_1, features_categorical_to_ordinal),\n",
    "        ('cat2', transformer_categorical_transformer2, features_categorical_to_onehot),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Training & Tuning Hyperparameters\n",
    "We'll cover three models for this study, starting from the simplest & fastest one, __\"Linear Regression\"__. Later we explore more sophisticated models, tree-based to be clear, __\"Random Forest\"__ & __\"XGBoost\"__.\n",
    "\n",
    "Hyperparameter tuning can be handled either automatically or manually. Actually there are many fantastic tools for automatic solutions if you google it. For the sake of simplicity we'll stick to manual approach and tune a couple of important parameters step by step, and one by one, __moving from the most significant to the least__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1)\n",
    "\n",
    "# bundle preprocessing and modeling in a final pipeline\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model_rf)\n",
    "])\n",
    "\n",
    "# preprocess train data & fit model\n",
    "pipeline_rf.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Score: 1251.7648342556438\n"
     ]
    }
   ],
   "source": [
    "y_pred_subsample = pipeline_rf.predict(X_valid_subsample)\n",
    "\n",
    "# preprocess validation data and get predictions to evaluate the model\n",
    "# don't forget to transfer back target values after inference prediction with np.expm1() ;)\n",
    "score_mae_rf = mean_absolute_error(np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "print('MAE Score:', score_mae_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<MAE Scores for Random Forest Model :: Study Parameter: Max Depth>>\n",
      "Val.\tScore\n",
      "3 \t 1430.521\n",
      "5 \t 1348.5531\n",
      "10 \t 1269.5775\n",
      "15 \t 1254.3561\n",
      "20 \t 1250.9533\n",
      "25 \t 1252.6426\n",
      "30 \t 1251.1197\n"
     ]
    }
   ],
   "source": [
    "tune_subject = 'Max Depth'\n",
    "tune_values  = [3, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "scores = list()\n",
    "print(f'<<MAE Scores for Random Forest Model :: Study Parameter: {tune_subject}>>\\nVal.\\tScore')\n",
    "for value in tune_values:\n",
    "    model_rf = RandomForestRegressor(max_depth=value, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    pipeline_rf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_rf)\n",
    "    ])\n",
    "    pipeline_rf.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "    y_pred_subsample = pipeline_rf.predict(X_valid_subsample)\n",
    "    score_mae_rf = mean_absolute_error(np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "    scores.append((value, score_mae_rf))\n",
    "    print(f'{value} \\t {round(score_mae_rf, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is the best one. We move on with max_depth=20 from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<MAE Scores for Random Forest Model :: Study Parameter: Number of Estimators>>\n",
      "Val.\tScore\n",
      "50 \t 1258.1482\n",
      "100 \t 1250.9533\n",
      "150 \t 1249.169\n",
      "200 \t 1248.4397\n",
      "250 \t 1248.6593\n"
     ]
    }
   ],
   "source": [
    "tune_subject = 'Number of Estimators'\n",
    "tune_values = list(range(50, 300, 50))\n",
    "\n",
    "scores = list()\n",
    "print(f'<<MAE Scores for Random Forest Model :: Study Parameter: {tune_subject}>>\\nVal.\\tScore')\n",
    "for value in tune_values:\n",
    "    model_rf = RandomForestRegressor(max_depth=20, n_estimators=value, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    pipeline_rf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_rf)\n",
    "    ])\n",
    "    pipeline_rf.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "    y_pred_subsample = pipeline_rf.predict(X_valid_subsample)\n",
    "    score_mae_rf = mean_absolute_error(np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "    scores.append((value, score_mae_rf))\n",
    "    print(f'{value} \\t {round(score_mae_rf, 4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue with n_estimators=200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<MAE Scores for Random Forest Model :: Study Parameter: Max Number of Features for Splitting>>\n",
      "Val.\tScore\n",
      "8 \t 1276.9501\n",
      "16 \t 1250.7871\n",
      "32 \t 1242.985\n",
      "40 \t 1241.9461\n",
      "48 \t 1240.6981\n",
      "56 \t 1239.9349\n",
      "64 \t 1242.188\n",
      "72 \t 1238.2277\n",
      "80 \t 1240.6245\n"
     ]
    }
   ],
   "source": [
    "tune_subject = 'Max Number of Features for Splitting'\n",
    "tune_values = [8, 16, 32, 40, 48, 56, 64, 72, 80]\n",
    "\n",
    "scores = list()\n",
    "print(f'<<MAE Scores for Random Forest Model :: Study Parameter: {tune_subject}>>\\nVal.\\tScore')\n",
    "for value in tune_values:\n",
    "    model_rf = RandomForestRegressor(max_depth=20, n_estimators=200,\n",
    "        max_features=value, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    pipeline_rf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_rf)\n",
    "    ])\n",
    "    pipeline_rf.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "    y_pred_subsample = pipeline_rf.predict(X_valid_subsample)\n",
    "    score_mae_rf = mean_absolute_error(np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "    scores.append((value, score_mae_rf))\n",
    "    print(f'{value} \\t {round(score_mae_rf, 4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings our RF tuning to an end. The best hyperparameters are:\n",
    "```\n",
    "max_depth=20\n",
    "n_estimators=200\n",
    "max_features=56\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Score: 1223.4211677762921\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRegressor()\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model_xgb)\n",
    "])\n",
    "pipeline_xgb.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "y_pred_subsample = pipeline_xgb.predict(X_valid_subsample)\n",
    "score_mae_xgb = mean_absolute_error(np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "print('MAE Score:', score_mae_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use __gpu__ for faster hyperparameter search. If you have a compatible gpu, enable this option by setting it enabled in following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_enabled = True\n",
    "\n",
    "tree_method_applied = 'gpu_hist' if gpu_enabled else 'auto'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<MAE Scores for XGBoost Model :: Study Parameter: Number of Estimators>>\n",
      "Val.\tScore\n",
      "20 \t 1220.7585\n",
      "25 \t 1212.3367\n",
      "30 \t 1205.01\n",
      "35 \t 1205.8681\n",
      "40 \t 1206.8926\n",
      "45 \t 1206.2272\n",
      "50 \t 1205.4982\n",
      "55 \t 1209.66\n",
      "60 \t 1208.4578\n",
      "65 \t 1208.4743\n",
      "70 \t 1209.7424\n",
      "75 \t 1210.0238\n",
      "80 \t 1210.8627\n",
      "85 \t 1212.1921\n",
      "90 \t 1215.8706\n",
      "95 \t 1218.2615\n"
     ]
    }
   ],
   "source": [
    "tune_subject = 'Number of Estimators'\n",
    "tune_values = list(range(20, 100, 5))\n",
    "\n",
    "scores = list()\n",
    "print(f'<<MAE Scores for XGBoost Model :: Study Parameter: {tune_subject}>>\\nVal.\\tScore')\n",
    "for value in tune_values:\n",
    "    xgb_params = {\n",
    "        'n_estimators': value,\n",
    "\n",
    "        'objective': 'reg:squarederror',\n",
    "        'nthread': -1,\n",
    "        'tree_method': tree_method_applied,\n",
    "\n",
    "        'seed': RANDOM_SEED,\n",
    "        'verbosity': 1,\n",
    "    }\n",
    "\n",
    "    model_xgb = xgb.XGBRegressor(**xgb_params)\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_xgb)\n",
    "    ])\n",
    "    pipeline_xgb.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "    y_pred_subsample = pipeline_xgb.predict(X_valid_subsample)\n",
    "    score_mae_xgb = mean_absolute_error(np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "    scores.append((value, score_mae_xgb))\n",
    "    print(f'{value} \\t {round(score_mae_xgb, 4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go. Our preferred n_estimators equals 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<MAE Scores for XGBoost Model :: Study Parameter: Max Depth>>\n",
      "Val.\tScore\n",
      "2 \t 1260.5824\n",
      "3 \t 1222.3614\n",
      "4 \t 1210.7759\n",
      "5 \t 1205.4412\n",
      "6 \t 1205.01\n",
      "7 \t 1207.8981\n",
      "8 \t 1227.9128\n",
      "9 \t 1250.1969\n",
      "10 \t 1247.2156\n"
     ]
    }
   ],
   "source": [
    "tune_subject = 'Max Depth'\n",
    "tune_values = list(range(2, 11, 1))\n",
    "\n",
    "scores = list()\n",
    "print(\n",
    "    f'<<MAE Scores for XGBoost Model :: Study Parameter: {tune_subject}>>\\nVal.\\tScore')\n",
    "for value in tune_values:\n",
    "    xgb_params = {\n",
    "        'max_depth': value,\n",
    "        'n_estimators': 30,\n",
    "\n",
    "        'objective': 'reg:squarederror',\n",
    "        'nthread': -1,\n",
    "        'tree_method': tree_method_applied,\n",
    "\n",
    "        'seed': RANDOM_SEED,\n",
    "        'verbosity': 1,\n",
    "    }\n",
    "\n",
    "    model_xgb = xgb.XGBRegressor(**xgb_params)\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_xgb)\n",
    "    ])\n",
    "    pipeline_xgb.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "    y_pred_subsample = pipeline_xgb.predict(X_valid_subsample)\n",
    "    score_mae_xgb = mean_absolute_error(\n",
    "        np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "    scores.append((value, score_mae_xgb))\n",
    "    print(f'{value} \\t {round(score_mae_xgb, 4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The depth with value 6 is our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<MAE Scores for XGBoost Model :: Study Parameter: Learning Rate>>\n",
      "Val.\tScore\n",
      "0.1 \t 1464.9177\n",
      "0.11 \t 1370.7784\n",
      "0.12 \t 1314.1458\n",
      "0.13 \t 1275.7434\n",
      "0.14 \t 1251.7331\n",
      "0.15 \t 1236.7471\n",
      "0.16 \t 1219.0251\n",
      "0.17 \t 1213.4802\n",
      "0.18 \t 1211.7185\n",
      "0.19 \t 1212.8984\n",
      "0.2 \t 1201.7133\n",
      "0.21 \t 1210.9485\n",
      "0.22 \t 1207.23\n",
      "0.23 \t 1199.7316\n",
      "0.24 \t 1203.3186\n",
      "0.25 \t 1202.6958\n",
      "0.26 \t 1205.0798\n",
      "0.27 \t 1210.61\n",
      "0.28 \t 1218.2772\n",
      "0.29 \t 1211.2089\n"
     ]
    }
   ],
   "source": [
    "tune_subject = 'Learning Rate'\n",
    "tune_values = [round(item, 2) for item in np.arange(0.1, 0.3, 0.01)]\n",
    "\n",
    "scores = list()\n",
    "print(f'<<MAE Scores for XGBoost Model :: Study Parameter: {tune_subject}>>\\nVal.\\tScore')\n",
    "for value in tune_values:\n",
    "    xgb_params = {\n",
    "        'eta': value,\n",
    "        'max_depth': 6,\n",
    "        'n_estimators': 30,\n",
    "\n",
    "        'objective': 'reg:squarederror',\n",
    "        'nthread': -1,\n",
    "        'tree_method': tree_method_applied,\n",
    "\n",
    "        'seed': RANDOM_SEED,\n",
    "        'verbosity': 1,\n",
    "    }\n",
    "\n",
    "    model_xgb = xgb.XGBRegressor(**xgb_params)\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_xgb)\n",
    "    ])\n",
    "    pipeline_xgb.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "    y_pred_subsample = pipeline_xgb.predict(X_valid_subsample)\n",
    "    score_mae_xgb = mean_absolute_error(np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "    scores.append((value, score_mae_xgb))\n",
    "    print(f'{value} \\t {round(score_mae_xgb, 4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal learning rate would be 0.23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<MAE Scores for XGBoost Model :: Study Parameter: Subsample Ratio>>\n",
      "Val.\tScore\n",
      "0.8 \t 1198.0412\n",
      "0.81 \t 1209.7342\n",
      "0.82 \t 1210.6296\n",
      "0.83 \t 1203.6598\n",
      "0.84 \t 1212.6996\n",
      "0.85 \t 1205.8743\n",
      "0.86 \t 1212.0091\n",
      "0.87 \t 1210.9594\n",
      "0.88 \t 1206.6629\n",
      "0.89 \t 1206.6421\n",
      "0.9 \t 1191.9369\n",
      "0.91 \t 1206.6319\n",
      "0.92 \t 1203.9524\n",
      "0.93 \t 1201.693\n",
      "0.94 \t 1199.5656\n",
      "0.95 \t 1207.9843\n",
      "0.96 \t 1203.9203\n",
      "0.97 \t 1205.1545\n",
      "0.98 \t 1208.0474\n",
      "0.99 \t 1205.0366\n",
      "1.0 \t 1199.7316\n"
     ]
    }
   ],
   "source": [
    "tune_subject = 'Subsample Ratio'\n",
    "tune_values = [round(item, 2) for item in np.arange(0.8, 1.01, 0.01)]\n",
    "\n",
    "scores = list()\n",
    "print(f'<<MAE Scores for XGBoost Model :: Study Parameter: {tune_subject}>>\\nVal.\\tScore')\n",
    "for value in tune_values:\n",
    "    xgb_params = {\n",
    "        'eta': 0.23,\n",
    "        'max_depth': 6,\n",
    "        'n_estimators': 30,\n",
    "        'subsample': value,\n",
    "\n",
    "        'objective': 'reg:squarederror',\n",
    "        'nthread': -1,\n",
    "        'tree_method': tree_method_applied,\n",
    "\n",
    "        'seed': RANDOM_SEED,\n",
    "        'verbosity': 1,\n",
    "    }\n",
    "\n",
    "    model_xgb = xgb.XGBRegressor(**xgb_params)\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_xgb)\n",
    "    ])\n",
    "    pipeline_xgb.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "    y_pred_subsample = pipeline_xgb.predict(X_valid_subsample)\n",
    "    score_mae_xgb = mean_absolute_error(np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "    scores.append((value, score_mae_xgb))\n",
    "    print(f'{value} \\t {round(score_mae_xgb, 4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, 0.9 for subsample ratio works the best here! This wraps up our XGBoost tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing The Best Model\n",
    "XGBoost performed the best among tuned three models discussed here. That will be our go-to model for final train on data and our train script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training on All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished :)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train_full[features_all].copy()\n",
    "y_train = df_train_full[name_of_target_column_transformed].to_numpy()\n",
    "X_test = df_test_full[features_all].copy()\n",
    "\n",
    "\n",
    "gpu_enabled = True\n",
    "tree_method_applied = 'gpu_hist' if gpu_enabled else 'auto'\n",
    "\n",
    "# tuned hyperparameters\n",
    "xgb_params_final = {\n",
    "    'eta': 0.23,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 30,\n",
    "    'subsample': 0.9,\n",
    "\n",
    "    'objective': 'reg:squarederror',\n",
    "    'nthread': -1,\n",
    "    'tree_method': tree_method_applied,\n",
    "\n",
    "    'seed': RANDOM_SEED,\n",
    "    'verbosity': 1,\n",
    "}\n",
    "\n",
    "model_final = xgb.XGBRegressor(**xgb_params_final)\n",
    "pipeline_final = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model_final)\n",
    "])\n",
    "pipeline_final.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = np.expm1(pipeline_final.predict(X_test))\n",
    "\n",
    "print('Training finished :)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Score: 1156.6538551622903\n"
     ]
    }
   ],
   "source": [
    "# test the final model on the same subsample validation bunch\n",
    "y_pred_subsample = pipeline_final.predict(X_valid_subsample)\n",
    "score_mae_subsample_final = mean_absolute_error(np.expm1(y_valid_subsample), np.expm1(y_pred_subsample))\n",
    "print('MAE Score:', score_mae_subsample_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, training on all data improved subsampled data score."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
